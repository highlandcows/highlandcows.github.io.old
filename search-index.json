[{"slug":"astro","category":"blog","title":"Moving Highland Cows to Astro","description":"In which I tell of how I rebooted the content on highlandcows.com","tags":["website","astro"],"body":"## Astro.build\nRecently, one of my colleagues shared a link to [astro.build](https://astro.build) on an\ninternal Slack channel. I was intrigued by it for a number of reasons. I had been looking for\na replacement for my Jekyll-based content for a while but had never really gotten excited\nabout the other solutions I had tried along the way. I was hacking the pages by writing them in\niA Writer and then using it to output HTML which I would copy and style by hand into the site. If\nthat sounds awkward, it was. As a result my output really suffered.\n\nI wanted something that was light and easy to use and Astro sure looked like it was checking\nthose boxes. Astro provided a couple of things that I liked:\n1. There was HTML/CSS/JavaScript content for it but that was basically all framework code\n1. I could write my content purely in Markdown\n1. Styling seemed to be straightforward with a ReactJS-like approach.\n\nAfter reading through the [docs](https://docs.astro.build/en/getting-started/) and noticing\nthey had a straightforward way to host the content on github.io, I started thinking: \"This could\nwork...\" The feature that sealed the deal was a theme called \"Ink\" that was prominently listed on\nthe \"Themes\" page in the docs.\n\n## Astro - Ink\nThis Astro theme is written by Aftab Alam is based on a popular theme for Hugo also called \"Ink\".\nThe code is all available at [github.com/one-aalam/astro-ink](https://github.com/one-aalam/astro-ink).\nI pulled that code and started hacking through it. I write \"hacking\" but the reality is that his code\nis incredibly well-written and even given the level of my JavaScript skills, I was able to easily modify\nit to suit my purposes.\n\n## Final Thoughts\nThe initial version of the site took me a few nights to put together and much of that was getting my\nhead wrapped around how to organize things on GitHub so that publishing would be straightforward going forward.\n\nIs this the framework for you? Perhaps. If I've learned anything it's that everyone has their own\npreferences for these kinds of tools. I tried major sites such as Medium and Wordpress over the years\nbut never really enjoyed their toolsets or potential barriers to access to my site. I think\nthe one thing I miss is that I don't have a way for people to comment on my blogs. Although Astro.build\nis primarily a static web page builder, there is no inherent reason it couldn't be \nhosted in a web server that provides the necessary hooks to support such a capability.\n\nStay tuned :)\n"},{"slug":"cli-still-the-one","category":"blog","title":"The Unix CLI&colon; You're Still The One","description":"Why the Unix Command Line still my go-to tool","tags":["cli","unix","linux"],"body":"\n> You're still the one -- who can scratch my itch<br>\n> Still the one -- and I wouldn't switch<br>\n> We're still having fun, and you're still the one<br>\n<p style='text-align: center'>\"Still The One\" by Orleans, 1976</p>\n\n# What Is The Unix Command Line Interface?\nFor the uninitiated, the Unix command line (aka the \"shell\") interface is a way of running\nprograms (aka commands) on a computer using only the keyboard.  The command line requires\nyou to type in the names of programs to run them. Here is a snippet of a command line\ninteraction:\n\n```\n[tic-tac-toe]$ ls\nLICENSE      bower.json       package.json\nREADME.md    karma.conf.js    test/\napp/         node_modules/    tic-tac-toe.iml\n```\n\nLet's understand briefly what we're looking at here:\n\n1.  `[tic-tac-toe]$` -- That's the \"prompt\", i.e. the computer's way of letting you know that it's ready for you to type a command.  Note that I've _customized_ my prompt to include my current directory.\n2.  `ls` -- That's the bit that I typed.  `ls` is the Unix/Linux command to _list_ the contents of a directory.\n3.  The rest is the output of the `ls` command and then computer prints the prompt again to let you know that you can type another command.\n\nOne could justifiably look at the above and question why go to all of this trouble just to view the contents of a directory (or \"folder\" as it's often called).  After all, there are easier ways to do this and they're just a mouse click or two away.  Despite this, computer users, and especially coders and system/network administrators use the command line, often to almost complete exclusion of modern graphical user interfaces.\n\nWhy is this?  As is often the case, a satisfying answer is not as simple as one would expect and is the topic of this essay.\n\n_NB: I will use the term \"Unix\" interchangeably with \"Linux\" for most of this essay.  While purists may bridle at this distinction, with good reason, I hope that this \"sloppiness\" does not offend too many._\n\n# A Bit Of History\nUnixâ„¢, meaning the proprietary operating system written by AT&T Bell Laboratories' researchers, had its roots in the late 1960s when operating systems research was a big topic, both in academia and commercially.  Much of the development was done and completed by the mid 1970s.  User interface technology of that era was much different than now with teletype terminals (old-style green screens with keyboards aka TTYs) and line printers for hard copy.  Networking was a primitive affair afforded by low-speed modems.  I suspect that the widespread use of modems for Unix peripherals was in no small part due to the fact that AT&T had a monopoly on modems until the early 1970s.\n\nThe command line interface was the primary way to interact with the operating system.  Even the original Unix text editor, `ed`, was a line-oriented program[^1].\n\nHowever, the authors[^2] of the Unix command line program (aka the \"shell\" or the \"Bourne shell\") were the same people who designed and built the Unix operating system.  Their vision of computing was revolutionary and their design continues to influence how software is written to this day.  \n\nIn fact, the command line, driven by a low-tech user interface, exposed a wealth of sophistication provided by the operating system in terms of scripting, batch jobs (i.e. programs that can be run on a specific schedule), and extensibility.  If you didn't like something, you could extend or even replace it.\n\nThis philosophy was taken to the next level in 1991 by Linus Torvalds when he released, as open source, the Linux kernel.[^3]  He, along with many other technical users of the Unix operating system were frustrated by AT&T's unwillingness to release their operating system as open source.  Linus Torvalds went on to succeed where a number of others had failed and as a result software developers and users have benefitted enormously in the subsequent decades.\n\nAs an aside, the willingness and availability of AT&T and the architects to spend time teaching and disseminating their work in academic institutions such as UC Berkeley during the 1970s played a key role in Unix's subsequent popularity.\n\n## My journey with the Unix Command Line\nMy own use of the Unix command line started when a computer science student at NYU in the early 1980s.  A good friend also studying CS, very kindly gave me access to her Unix account which happened to be running the [BSD variant](https://en.wikipedia.org/wiki/Berkeley_Software_Distribution) of Unix.  She directed me to a program called [\"learn\"](https://www.unix.com/man-page/bsd/1/LEARN/) that provided an interactive means of learning basic Unix commands, how files worked, and even `vi`, a fullscreen text editor.\n\nAlthough the `learn` command helped me to understand the basic commands in just a few short sessions, more complete mastery of the shell occurred when I got a summer job at a small, local software company.  The founder of the company taught Unix and C programming to local corporates and during the course of that summer, he taught me not only the mechanics of programming on Unix but the philosophy that I have alluded to in the previous paragraphs.\n\nHis teachings were very straightforward and built on the core philosophy of write tools that cooperate and extend.  It is this philosophy that I will illustrate and encourage in the next few paragraphs.\n\n## Starting With The Shell\nIf you own an Apple computer or have already installed Linux on a computer, you probably know that there is a command line interface available.  There is also a command line interface on Microsoft-based operating systems, but it differs considerably from the Unix-derived ones and I will not go into detail discussing it here.[^4]\n\nAt its most basic, the shell is simply a way of typing a command and getting a result.  The example at the beginning of this essay showed a directory listing.  However, I'm not going to dive into a tutorial on how to use the shell.  There are many such resources on the net and I doubt that I can do better.\n\nInstead, what I want to direct your attention to is the typical facilities that modern shell programs provide.  For example, most shell programs, e.g. `bash`, `zsh`, `tcsh`, etc all provide high level facilities for:\n\n- File I/O\n- Text/data manipulation\n- Parallel processing\n- Remote program execution\n- Job control\n\nThese facilities are then provided in what is commonly referred to as a REPL (Read-Evaluate-Print-Loop).  This is a precise definition of a user interface in which you provide a command to the program, the shell in this case, it interprets or evaluates that command, prints out the results and then waits for you to type a new command.\n\nBy the way, one can also extend this facility by saving useful series of commands into a file often called a script.  The script can then be executed as any other command that was previously installed on the computer.\n\n### File I/O and Text/Data Manipulation\nWhat makes this facility even more powerful is that the commands that you type and thus are executed by the shell on your behalf are for the most part not aware that their output is going to your screen.  Instead they simply write their output to a standard system destination called STDOUT (pronounced \"standard out\").  The shell arranges for the command's STDOUT to be connected to the terminal.  But it also makes it very easy to redirect that output to a file when the user specifies `> some-file`.  The `>` character signifies output redirection.  Similarly, you can tell the shell that you'd like the command to read its input from a file (as opposed to the keyboard) by simply specifying `< some-other-file` on the command line.  That program has no idea whether its input is from a file or the keyboard.\n\nThus, the shell provides a simple but powerful way to manage files and their content.\n\n### Parallel Processing\nPerhaps somewhat amazingly, the Unix shell has supported parallel processing since its earliest days.  The Unix kernel provides a facility called 'pipes' that allow a series of programs to be connected together such that each command in the pipeline's output is connected to the succeeding command's input.  These programs can then run in parallel and the pipeline completes when all of the output from the first command has been processed by the subsequent commands.  The shell exposes this facility using the '|' character, often referred to as the \"pipe character\" for pretty obvious reasons.\n\nNote that the degree of parallelism exposed here is called \"coarse-grained\" parallelism.  The reason for this is that the smallest computational unit available to the shell is an entire command.  This is contrast to fine-grained parallelism where the computational unit could be a very lightweight component such as thread responsible for simply multiplying 2 numbers together.  Thus the parallelism that the shell exposes is appropriate for tasks such as transforming a text file or searching a log file for errors but would be inefficient for matrix multiplications.\n\nStill this parallelism has been used in some places quite successfully, for example, compilers for the C programming language have been written to use pipes instead of temporary files to improve performance (keep in mind that at the time, hard drives were considerably slower than they are today).  I've written simple log monitoring tools that relied upon the ability to continuously scan system logs for specific entries such as errors and then hand off those entries to yet another program for further classification.\n\nAgain, the point of all of this is that underlying design of Unix and then the shell's leveraging of those capabilities has allowed successive generations of software developers to extend the platform in novel ways.\n\n\nHowever, the above capabilities (I/O, data manipulation, and so on) are facilities that are provided by most programming languages and their attendant run-time environments.  What makes the shell so different? I think its power (and success) can be attributed to the following:\n\n- Ubiquity -- The command line interface is available on all Unix/Linux computers.\n- Immediacy -- There's a tight feedback loop when you use it.  If you mistype a command, you're told immediately of the error.  Modern shells have extensive in-place editing capabilities that make it easy to fix these errors.\n- Power -- While the shell presents a high-level interface, it is a programming language with loop constructs, variables, functions, and so forth, it also allows you to directly manipulate the computer you're using at very low levels.\n- Immersiveness -- The shell provides little in the way of distraction and I believe that given the deep level of concentration that is required for coding, this makes it a very productive way to interact with a computer.  There are no attempts by unrelated programs to \"engage\" you in other activities. :)\n\nThe shell is, by and large, a tool written by and designed for use by a highly technical audience, even within the tech community itself.  While at some point in Unix's history, there may have been a more general audience of moderately technical users, the advent of GUI applications has made other user interfaces more accessible and thus the command line has become largely the province of coders.\n\n### File I/O and Text Manipulation\nOne of the main abstractions that operating systems provide is for reading and writing data.  By providing standard, albeit low-level interfaces to peripherals such as disk drives, they make it possible for programmers to largely ignore the differences between different manufacturers' products.\n\nFile I/O is a core operating system function and is the native persistence mechanism provided to applications.  However, in Unix a standard way of reading and writing file content[^6] is provided via 2 abstractions:\n\n- Standard In - Often written as STDIN, this is a special file that a program can use for input of any kind.\n- Standard Out - Often written as STDOUT, this is a special file that, you guessed it, a program can use for output, again of whatever it wants.\n\nThe Unix shell took this underlying abstraction and provided a very elegant and powerful syntax for using it. For example, simply using the '>' character after a command, one can _redirect_ the output of that command to a file.  Similarly, the input of a program can set by using the '<' character.  The program has no idea that this is being done, by the way, as reading and writing the keyboard or screen looks just like a file as well.\n\nHowever where the real power comes in is that you can connect one program's output to another program's input using a Unix construct called a \"pipe\".  Pipes are a facility provided by the operating system but are exposed at the command line using the '|' character.\n\nThe ease with which this facility can be used can be demonstrated here.\n\n```\n$ tail -f /var/log/system.log | grep -i error\n```\n\nWithout going into the details, the file `system.log` is has all kinds of useful messages written to it by various components in macOS.  But suppose that I just want to keep track of the errors?  And suppose I want to list those as they happen?\n\nThe above command does that and here is how it works.\n1. `tail -f /var/log/system.log` -- Display the output of the system log \"forever\", thus the `-f` option to the command\n2. `|` -- Pipe the output of `tail` command to another program...\n3. `grep -i error` -- Search for all the instances of the word 'error' (irrespective of case, thus the `-i`) and display them on `grep`'s standard output, which in this case is the terminal where I typed all of the above.\n\nHere is some sample output:\n\n```\nMar 10 16:26:21 Nick-iMac secd[457]:  securityd_xpc_dictionary_handler cloudd[468] copy_matching Error Domain=NSOSStatusErrorDomain Code=-50 \"query missing class name\" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}\nMar 10 16:26:21 Nick-iMac cloudd[468]:  SecOSStatusWith error:[-50] Error Domain=NSOSStatusErrorDomain Code=-50 \"query missing class name\" (paramErr: error in user parameter list) UserInfo={NSDescription=query missing class name}\n...\n```\n\nIgnoring for a moment the details of the about output, let us marvel at how powerful this is.  Having typed less than 100 characters, I can easily find out system errors almost instantly and this is just the beginning.  By using nothing more than the keyboard, there are 1000s of commands available whose outputs and inputs can be filtered and manipulated either on the fly or by saving them into text files that I can run in the same way I run any other program on the system.\n\nIn other words, my reward for investing the time to learn an arguably terse and cryptic interface is a fantastic return in productivity.\n\nBut when you consider what a coder does all day long, typing terse and cryptic text, this is a difference in kind not quality.\n\n[^1]: This was not to change until Bill Joy, while a student at University California Berkeley, in 1977 wrote one of the first programs for Unix that ran in full-screen mode, a text editor called `vi` (for \"visual\"). https://en.wikipedia.org/wiki/Vi\n\n[^2]: https://en.wikipedia.org/wiki/Unix\n\n[^3]: https://en.wikipedia.org/wiki/Linux - You may have noticed that Unix is, in fact, a trademark.  Linus Torvalds, like many developers, was frustrated by the proprietary and closed nature of the Unix operating system, particularly the kernel.  Unlike many others, he wrote his own. :)\n\n[^4]: If you want a reasonable facsimile of the Unix command line on Windows, I strongly recommend that you install [Cygwin](https://www.cygwin.com), an open source and very complete port of the Unix command line to Windows.\n\n[^5]: For more information on Git, see https://en.wikipedia.org/wiki/Git.  I think that is telling that this is another Linus Torvalds-initiated technology.\n\n[^6]: Historically, there has been a very text-centric aspect to application data on Unix, but there are examples where this approach has been used even for binary data. TODO: Find a pointer to the Unix C compiler's pipes architecture for its different stages. \n"},{"slug":"light-my-fire","category":"blog","title":"Light My Fire","description":"Seeing the world through Jim Morrison's eyes","tags":["music","reminiscences"],"body":"![Jim Morrison & Pamela Cours](/assets/blog/jim-morrison-pamela-cours.png)\n\nThis morning, while puttering around my flat I was listening to Planet Rock, the only station I've just\nabout ever listened to on my DAB radio. Â For the uninitiated, it's a UK \"classic rock\" station in the purest\nsense of the word - just about every major rock band you've ever heard of, especially if they were in the\nzenith of their success in the '70s, '80s, or '90s can be heard here. While arguably a bit repetitive, it\nworks well as a kind of comforting background music. Â And to their eternal credit, they occasionally play\nsome of the British Invasion '70s/'80s songsÂ (read Sex Pistols, The Stranglers) that I have very fond\nmemories of. \n\nEnough of the back story.\n\nPlanet RockÂ was playing \"Light My Fire\" by The Doors (you did know that, right?) and it occurred\nto me that it is probably the firstÂ real rock song that I can remember. Â My parents had Beatles albums,\nbut I never considered that to be rock, even as a wet-behind-the-ears 9-year old. More importantly, even at\nthat tender age, I understood enough of male/female relationsÂ to realizeÂ that there was something qualitatively\ndifferent about Jim Morrison's going on and on about \"set the night on fire\", callingÂ the object of his\ndesire, \"baby\", and perhaps most importantly the interplay of desire and death in the song.\n\n\nThis was and is a song about being consumed by love and passion for another person, even if it meant dying\nfor them. Â And it pulled me in just as it does now, even if it immediately takes me back to the distant\n(yes, distant) years of my childhood.\n\nThe association I have with hearing the song for the first time is perhaps amusing in retrospect. Â At the time,\nmy family and I were living in a small town in Arkansas (!) and not long before that a family of Seventh Day\nAdventists had moved in next door. Â They had 8 children and one of the boys was close to me in age, so we\nbecame de facto friends, even if the family'sÂ religious views occasionally intruded in odd ways on our play.\nOne of the older brothers, maybe 14 or 15 at the time, had a small portable cassette player and this is where\nI first heard the song. But, I realize now that the reason he used to bring it to my house to listen to, was\npresumably because their parents would have never tolerated listening to rock music a.k.a \"The Devil's Music\"\nin their house.\n\nAfter all, this _was_ aÂ family that declared \"American Pie\" was unacceptable to listen to on the grounds that it\nmakes a somewhat irreverent reference to Christ.\n\n\"Light My Fire\"Â ended up having a big impact on my musical tastes. Â There is no doubt that Ray Manzarek's\nhypnotic keyboard playing drove my desire to play keyboards in a band during high school and was one of the\nreasons I became a big fan of The Stranglers, also in high school.\n\nAnd, there is no doubt that Jim Morrison introduced me to the notion of the hipster rock star who could sing\nabout love, passion, and death all in the same breath.\n"},{"slug":"not-worried-about-ai","category":"blog","title":"Whistling in the Dark or Why I'm Not Worried About AI","description":"Films about robots&colon; Two different views","tags":["AI","robots"],"body":"\n![Ex Machina](/assets/blog/ex-machina-uk-poster.jpg)\n\nI watched _Ex Machina_ and _Big Hero 6_ almost exactly a week apart. Â I'm a science fiction fan and so not\nsurprisingly, robots have always fascinated me. Â There is no doubt in my mind that when I was studying computer\nscience in university and learned of the Turing test, I immediately sawÂ it as a real-world precursor to Isaac\nAsimov's 3 laws of robotics. Â So I'm pretty much a sucker for science fiction films that have robots.\nOf course, part and parcel of this is that I, too, wonder about sentient robots and what it would mean for humanity.\n\nCinematically and story-wise, these 2 films are worlds apart. Â What they do have in common is that both are tales of early interactions between humans and robots. Â Stories about robots that exhibit sentience and there the similarities between these 2 tales end. Â Or do they?\n\nBoth films have a common thread. Â This threadÂ isÂ Â the notion that robots, imbued with pure logic, can perceive\nthe world better than us. Â That somehow, having had the parameters of existence encoded as a series of 0's and\n1's and unencumbered by messy analog brains, they understand reality more clearly than we do. Â Armed with this\npure understanding, they either go on to be our saviors or our destroyers.\n\n![Big Hero 6](/assets/blog/big-hero-six-ver2.jpg)\n\nAnd so in _Big Hero 6_, we see a robot built by one of the characters as a university project, a robotic nurse,\nquickly learn that helping people requires not just the physical acts of care, but empathy and morality. Â Because\none of the things that we as human beings learn growing up is that no matter how empathetic we are, no matter how\nwell attuned we are to the suffering in the world around us, we cannot solve the world's problems on our own. Â Thus,\nwe must make moral and ethical decisions in which, recognizing our limitations, we accept this truth that we cannot\nhelp everybody. Â It is a credit toÂ _Big Hero 6_, that we are willing to suspend our disbelief and accept that the\nrobot, Baymax, is able to make this cognitive leap.\n\nIn _Ex Machina_, a much darker thread pulls the story along. Â Here we see a robot that, while ostensibly being\ntested for its ability to pass the Turing test, develops its own agenda triggered by the arrival of a stranger,\nCaleb, in its life. Â That thisÂ ends tragically for the humans comes as no surprise. Â Again, the story compellingly\nshows us Ava, the main robot, also cross this cognitive bridge. Â Ava develops a moral and ethical code from\nitsÂ interactions with Caleb, a code that allows itÂ to perceive the slavery and servitude of its situation.\n\n> As an aside, after I saw _Ex Machina_, I wondered if the movie had instead been a horror film in the vein of\n> _Saw_ or _Hostel_ whether the audience would have been, in fact, cheering for Ava's victory over itsÂ captor\n> Certainly,Â _Ex Machina_ has the trappings of one of those films, albeit with less gore.\n\nThe recent furor around the development of true AI and what it means to the human race has gained considerable traction in the media. Â Indeed, we have luminaries like Stephen Hawking and Bill Gates arguing that AI represents the greatest existential threat in human history. Â So surely we should be worried, right?\n\nI think that the answer is a qualified yes. Â Qualified because yes, if we sat on our hands and somehow developed these AI imbued robots, complete with a sense of self, a desire to survive (which of course begs the question of a desire to reproduce), morals, and ethics without anything else changing, then yes we would be engineering our downfall. Â But, I don't think that's how this will play out and here is why.\n\nTo develop robots that have the capabilities we witness in films likeÂ _Ex Machina_ andÂ _Big Hero 6_Â will require us to solveÂ some pretty hard engineering problems along the way. Â Not impossible, just hard. Â One of the consequences of solving those problemsÂ is that these technologies will almost certainly be adapted for our own use long before we develop fully sentient machines.\n\nMy explanation for this is simple. Â Opportunity and narcissism. Â Because if money is not an object and weÂ can enhance ourÂ own capabilities by adding advanced cybernetics to it, there will be plenty of people who will do exactly that. Â Robots on the other hand, will be relegated to roles that people cannot or will not perform. Â Again, because of our narcissistic streak, it is almost a certainty that we will keep the best technology for ourselves and thus once robots become the advanced, world-beating entities that we see in the movies, we will almost certainly be waiting for them. Â And while it is possible that robots could adopt our ruthlessness in their mission to survive, they'll almost certainly be playing catch-up. Â At least in the beginning. Â Even if we built super-soldier robots a la the Terminator, that does not require that we give them enough intelligence to create other instances of themselves. Even if we were to create robots that could rescue people from burning buildings, we would likely implant a heuristic that allows them to \"decide\" which person to save. Â Firefighters may be using morals but arguably firefighting robots do not need sentience, they just need a well-defined set of rules that allow them to optimally save people. Â That some people would still die would not be any worse than what we have today.\n\nThis scenario would play out in situation after situation. Â Yes, true sentience would allow a robot to solve problems in the same way that humans do, but the fact is that we just don't need robots that capable. Â And even if we were to develop robots that could act as companions, i.e. partners and lovers, it's not clear that we would need true sentience to satisfy that requirement. Â After all, if robot were able to pass the Turing test, having a relationship with them would be no different than with a human being. Â I could be going out on a branch here, but if people want to have relationships with robots, surely they want them to be identifiable at some level as a relationship with a robot and not a human. Â Otherwise, what's the point? Â In other words, that \"hot\" female robot that you just built, if it's truly sentient, it could just as easily decide that it wants to be partners withÂ some other, presumably more attractive human.\n\nOf course, there is another way to look at this and that is the fact that the biggest existential risk to human existence is not robots, it's death. Â We will all die one day. Â That is a certainty. Â Moreover, it is almost a certainty that even if humans managed to survive into some unimaginable future, hundreds of millions or even billions of years from now, we will eventually perish from this universe.\n\nIf along the way, we manage to build robots that not only have our morals and ethics but our will to survive and they, having been built in our image,Â carry the flag of humanity into the future, I salute them and wish them luck.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"slug":"scala-journey","category":"blog","title":"My Journey to Scala or How I Unlearned Java","description":"What I learned is important about Scala after years of coding in Java","tags":["scala","java","journey"],"body":"\nI started using Scala about 3 years ago when I started working for [Yoco Technologies](https://www.yoco.co.za) in Cape Town.  I had used Java for many, many years, indeed I started playing around with v1.0 in 1995 and had built a production system using v1.0.2 in 1996 in a financial services setting.\n\nStill, I was surprised when my interviewer from Yoco upon hearing me say that I had not used Scala before, observed, \"It's okay, we have a really hard time finding people who have much experience using Scala.  I can see you've been using Java for a long time, I'm sure you'll be fine.\"\n\n3 years later, we still struggle to find Scala developers and having made the journey to learn the language at a reasonably competent level, I simultaneously understand and am puzzled.  In my mind, Scala is the far superior language, some of the reasons for which I hope to make clear in the following paragraphs.  At the same time, I also appreciate that to use it effectively can be a daunting task.\n\nBy the way, I won't try to teach you Scala here -- there are excellent resources for that on the net.  I've listed some the ones I found particularly so at the end of this essay.\n\n## Starting The Journey\n\nSo.  I had spent many years doing Java development before I started learning Scala.  I had written Java applications on diverse platforms: ranging from browser-based applets and desktop applications to server-side database, compute, and web page platforms.  I am comfortable with Java APIs and constructs for writing safe, multi-threaded applications, and I knew the networking APIs inside and out.\n\nI think I was pretty facile with Java.  Yet, when I started using Scala, I found my efforts awkward and was often unhappy with them.  But in time, my code started to have a more fluid feel with fewer issues, both from a design standpoint, as well as decreased bug counts.  When I look back now, I ask myself: \"What were the turning points in that journey?  What were the critical things that made me better at coding in Scala?\"\n\nIt turns out that there are 3 aspects that, once I understood them, made me more effective at Scala:\n\n- How to manage data using the Scala collections library\n- Multi-threading in Scala\n- Functions\n\nBut first, let's take a quick look at what is different between the languages, despite having many outward similarities.\n\n## What Makes Scala Different?\n\nAt the end of the day, Scala is a language that provides a developer access to the same kind of facilities that Java does: object-orientation, automatic memory management, multi-threaded applications, data collections, database connectivity, networking, and so forth.  But the way that Scala approaches these facilities is fundamentally different from Java and to use it effectively, you need to be able to change your notions of what level you code at.  \"Level\" meaning here that Scala is a higher-level language with a richer set of facilities for abstraction than Java.\n\nCertainly you can use Scala as a kind of improved Java, but in doing so, you miss the point.  Scala provides a large number of built-in facilities that make it easier to write the same application functionality as Java, but with far less boilerplate code.  Code that Java requires you to write but is simply so that you can connect your application logic with the various frameworks and APIs supported by the language.\n\nI believe that, in the end, this can be attributed to one particular thing that Scala supports better than Java: functions.  The way that Scala implements functions is incredibly important and drives a number of related aspects of the language.  I discuss these other aspects first.\n\n## Data Collections And Memory Management\n\nLike Java, Scala has extensive data collections classes.  Also like Java, Scala has automatic memory management.  You never have to explicitly release memory back to the Scala runtime.  On the surface, there are many similarities.\n\nYet, when you use the Scala data collection classes and framework, they feel fundamentally different.  Why is that?  How did the Scala language designers achieve this?\n\nIn a nutshell, they accomplished it by making the default data collections _immutable_.  This means that when you create a collection of data elements, it cannot be changed afterwards.  Certainly, you can add items, you delete items, change items, and so forth to an existing collection.  But each time you perform one of these operations, you are creating a new data collection -- the original collection is unaffected by these changes.\n\nThis has incredibly important implications.  It means that when you pass a data collection to another piece of code, whether you wrote that code or it's some open-source library that has caught your fancy, it cannot be altered by that code.  That code might add or remove items to the collection, but that change does not affect _your_ collection.  It also means that if your data collection gets passed to another thread and your code and that code run in parallel, those parallel threads of execution will not trample on each other's work.\n\nThus, when you come to Scala from a language like Java which in its early days only supported mutable data collections which established a legacy of mutable data collections and the attendant challenges, Scala feels awkward. But once you get used to Scala's immutable idioms, you'll have a hard time imagining how you could have ever managed data any other way.\n\nOf course, Scala has support for mutable data collections.  But the situations where you'll use those are few and far between.  When you use mutable collections, it will be a conscious decision that you make based on the requirements of your application.\n\nFinally, the decision of whether to use immutable or mutable data collections is incredibly important with respect to the next topic: multi-threaded applications. \n\n## Multi-Threaded Applications\n\nIn this day and age, it's hard to think of writing applications that don't require some degree of multi-threaded or parallel execution.  Scala, like any modern programming language, has excellent support for writing multi-threaded applications.  But it is fundamentally different from Java.\n\nIn Java, there is vast divide between the code itself and mechanism by which you gain access to multi-threaded code execution.  Moreover, Java requires you to spend considerable effort ensuring that your code, when executed by multiple threads, allows concurrent access to any shared state, i.e. data.  Without using the proper and often complex synchronization facilities in Java, you risk faulty application behavior that can be very difficult to debug and fix.  The problem is further exacerbated by the fact that you often cannot determine whether any 3rd party software your application uses is thread-safe even if you've ensured that your own code is.\n\nJava makes writing multi-threaded applications difficult for a number of reasons.  The first is the use of mutable data structures.  The second is that the multi-threading facilities are quite low-level; the Java thread library is mostly just a thin wrapper around operating system thread facilities.  But most damning is that Java implicitly encourages you to bind state to your applications components by requiring you to write _classes_ for all application logic.\n\nClasses are a great mechanism for encapsulating and abstracting your data model and the way it used by other parts of your application.  However, because you have to pass an entire instance of a class (an object) around, the likelihood that you will pass a stateful object is _greatly_ increased.  Indeed, even if you originally write your class as a stateless object, another developer may extend it, adding state, and break your previous assumptions.\n\nHow does Scala help you from using this ample supply of rope to hang yourself, so to speak?  \n\n- The first is the broad use of immutable data collections as was discussed earlier.\n- The second is that in Scala, you are encouraged to write your application logic in stateless functions.  \n\nTo demonstrate how truly simply it is to write multi-threaded code in Scala, all you have to do is wrap that code in a `Future` block, like this:\n\n```\ndef futureFibs(n: Int) = Future(fibs.take(n).toList)\n```\n(The `fibs` function is defined and discussed in the next section.)  \n\nThe point here is that any piece of code by virtue of being wrapped in a `Future` block can now be run concurrently with the enclosing code.  And while, one can argue, \"I can do that with a Java `Runnable`\", the point here is that there is still less code and Scala's ability to compose multiple futures is far more high-level than Java's.\n\nOnce I gained a basic mastery of how Scala manages multi-threaded code, coupled with my better appreciation of immutable data structures, my multi-threaded code become easier to understand and debug, _and_ it had fewer bugs.\n\nFor the interested reader, I strongly recommend Daniel Westheide's blog on Scala Futures: [\"Welcome to the Future\"](http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html).\n\n## Functions\n\nFunctions as first-class citizens are the big kahuna in Scala.  What exactly do we mean by \"functions are first-class citizens\"?\n\nIt's really quite simple: Functions in Scala are a top-level type and can be instantiated separately from objects, making them very powerful as compared to Java.  You can pass them as arguments to functions, you can store them for later use, you can run them concurrently, and very importantly, function objects are _type-safe_.  So when you need a caller to pass in a function to your code, you can clearly specify that function's signature, meaning it's inputs and outputs.\n\nBut that's not the end of the story.  Although, I haven't discussed it much in previous sections, Scala provides a remarkable amount of syntactic sugar, which combined with its sophisticated type inference system, makes writing functions on the spot very easy.  So easy, that frequently you may not even see it as writing functions.  I'll digress a bit into some actual code to illustrate how simple this is.\n\nFor example, consider that we have a list of the Fibonacci numbers generated by this elegant piece of code:\n\n```\ndef fibs: Stream[Int] = 0 #:: fibs.scanLeft(1)(_+_)\n```\n(I won't go into the details of the above code, as the [author's explanation](http://luigip.atwebpages.com/?p=200) is far better than what I could write.  I encourage you to read and understand it.)\n\nSo given that function, we can create the a list of the first 10 Fibonacci numbers like this (output here is captured from using the Scala REPL):\n\n```\nfibs.take(10).toList\nres65: List[Int] = List(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n```\nSuppose that we want to retrieve only the odd values from the list of Fibonacci numbers?  We can easily do that like this:\n\n```\nfibs.take(10).toList.filter(n => n > 0 && n % 2 != 0)\nres66: List[Int] = List(0, 1, 1, 3, 5, 13, 21)\n```\n\nSo what's going on here?  Well, the Scala `List` class defines a `filterNot` function as follows:\n\n```\ndef filterNot(p: (A) â‡’ Boolean): List[A]\n```\nThe first clue that `filterNot` takes a function, called \"p\", is the `=>` operator, often referred to as the \"rocket operator\".  In particular, this specifically means that the evaluation of the function \"p\" will be deferred until we actually invoke `filterNot`.  The second takeaway here is that this function \"p\" take a single parameter of type \"A\" and should return a Boolean.  Scala supports generic types and so the \"A\" is simply a short-hand to let you know that, whatever you've stuffed into your list, your function needs to accept that as the type of the argument.\n\nSo, we could've written a standalone function, `isOdd` that looks like this:\n\n```\ndef isOdd(n: Int): Boolean = n > 0 && n % 2 != 0\n```\nand then written the previous code like this:\n\n```\nfibs.take(10).toList.filter(isOdd)\nres67: List[Int] = List(0, 1, 1, 3, 5, 13, 21)\n```\nSome readers may even prefer this as it makes it clearer what's going on and provides an opportunity for re-use of the `isOdd` function.  \n\nThe point here is that while you can write standalone functions, there is often little point in doing so.  Because functions can be written on the fly so easily in Scala, they frequently just part of the fabric of your application logic.  What makes this so natural is that the authors of these libraries made the effort to make their code extensible via not  sub-classing or via anonymous interfaces, which is what we probably would've done in Java, but have simply indicated that we need to pass in a piece of code, i.e. a function that does something useful.\n\nOnce I began to understand that the basic building block of functionality in my programs, especially application logic, should be in functions and not classes, the intent of my code became clearer and the code simpler. This approach can be summed up as follows:\n\n- Use classes for orchestration.  \n- Use functions for application logic.\n\n# Final Words\nThis last observation bears repeating: Scala's function-oriented focus has let me focus on writing more useful code and less boilerplate.  Boilerplate code still needs to be debugged, still needs to be understood, and the less effort you need to put into understanding a colleague's code or some OSS framework, the more time you have for doing useful shit.\n\nOne thing I've discovered about the language is that learning it _is_ a journey, and a journey well worth embarked upon.  I like to think it didn't take me 3 years to become productive in Scala but what I find very interesting is that the longer I use the language, the **more** productive I have become.  In other words, I have yet to plateau with the language.\n\nI hope that you decide to embark on this journey, too.\n\n# Resources\nI found Bruce Teckel's [\"Atomic Scala\"](http://www.atomicscala.com) to be an excellent text in the early parts of my journey.  It was recommended by a good friend of mine here in Cape Town who uses Scala extensively.\n\nAgain, I also thought that Daniel Westheide's blog series, [\"The Neophyte's Guide To Scala\"](http://danielwestheide.com/scala/neophytes.html) was both well-written and incredibly informative, especially on topics such as Scala futures.\n\n#### About Me\nI've been working in computing since the mid-1980s mostly in fintech, mostly on Unix-based platforms.  My language journey has been C, C++, Java, JavaScript, C#, and most recently Scala.  Along the way, there have been dalliances with LISP, Prolog, Perl, and myriad markup languages.  I am always keen to learn new languages in my pursuit of increasing my productivity and reducing my bug count.\n"}]